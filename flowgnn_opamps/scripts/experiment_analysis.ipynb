{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b30623-8600-4193-8ed5-d76f2c194b4e",
   "metadata": {},
   "source": [
    "# Experiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92c1ffc-37fb-4a00-b76c-16b44ba9ad69",
   "metadata": {},
   "source": [
    "### Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d347dcb8-8eb3-46be-8ad5-c47575b3f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "################################\n",
    "# SELECT MODELS ################\n",
    "################################\n",
    "model_names = [\"daflowgnn-2\"]\n",
    "props = [\"gain\"]\n",
    "runs = [1]\n",
    "###############################\n",
    "###############################\n",
    "###############################\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.grid()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "\n",
    "for model, prop, run in zip(model_names, props, runs):\n",
    "    \n",
    "    path = f\"./../experiments/supervised/{model}/{prop}/run_{run}/\"\n",
    "    results = pd.read_csv(path + \"results.csv\")\n",
    "    results = results[results[\"train_rmse\"] > 0]\n",
    "\n",
    "    plt.plot(results[\"train_rmse\"], label=f\"{model}_{prop}_{run}_train\")\n",
    "    plt.plot(results[\"val_rmse\"], label=f\"{model}_{prop}_{run}_val\")\n",
    "    plt.plot(results[\"test_rmse\"], label=f\"{model}_{prop}_{run}_test\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e707280a-9c28-49d3-a4e2-db1c2b17c092",
   "metadata": {},
   "source": [
    "### Display Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4822195-3fbe-4ebc-bda3-f6eb29c088cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "################################\n",
    "# SELECT MODELS ################\n",
    "################################\n",
    "models = [\"flowgat\", \"flowgatv2\", \"flowtransformer\", \"daflowgnn-1\", \"daflowgnn-2\"]\n",
    "props = [\"gain\", \"bw\", \"fom\"]\n",
    "################################\n",
    "################################\n",
    "################################\n",
    "\n",
    "leaderboard = np.zeros((len(models), len(props)*2))\n",
    "patience = 20\n",
    "\n",
    "leaderboard_models = []\n",
    "for i, model in enumerate(models):\n",
    "\n",
    "    model_dir = f\"./../experiments/supervised/{model}/\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        print(f\"{model} does not exist!\")\n",
    "        continue\n",
    "    leaderboard_models.append(model)\n",
    "    \n",
    "    for j, prop in enumerate(props):\n",
    "\n",
    "        prop_dir = model_dir + f\"{prop}/\"\n",
    "        if not os.path.exists(prop_dir):\n",
    "            continue\n",
    "\n",
    "        scores = []\n",
    "        for run in range(1, 11):\n",
    "            run_dir = prop_dir + f\"run_{run}/\"\n",
    "            if not os.path.exists(run_dir):\n",
    "                continue\n",
    "                \n",
    "            results = pd.read_csv(run_dir + \"results.csv\")\n",
    "            results = results[results[\"train_rmse\"] > 0]\n",
    "            scores.append(results[\"test_rmse\"].values[-(patience + 1)])\n",
    "\n",
    "        mean, std = np.mean(scores), np.std(scores)\n",
    "        leaderboard[i, 2 * j] = mean\n",
    "        leaderboard[i, 2 * j + 1] = std\n",
    "\n",
    "leaderboard = leaderboard[np.any(leaderboard > 0, axis=1)]\n",
    "leaderboard = np.round(leaderboard, 3)\n",
    "columns = []\n",
    "for prop in props:\n",
    "    columns.append(f\"{prop}_mean\")\n",
    "    columns.append(f\"{prop}_std\")\n",
    "    \n",
    "leaderboard = pd.DataFrame(leaderboard, columns=columns, index=leaderboard_models)\n",
    "if len(props) > 1:\n",
    "    leaderboard[\"mean_score\"] = np.round(leaderboard[[prop + \"_mean\" for prop in props]].mean(axis=1), 3)\n",
    "    leaderboard = leaderboard.sort_values(by=\"mean_score\")\n",
    "else:\n",
    "    leaderboard = leaderboard.sort_values(by=props[0] + \"_mean\")\n",
    "    \n",
    "leaderboard = leaderboard.reset_index()\n",
    "leaderboard.index += 1\n",
    "leaderboard = leaderboard.rename(columns={\"index\": \"model\"})\n",
    "\n",
    "display(HTML(leaderboard.to_html()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cktgnn-env",
   "language": "python",
   "name": "cktgnn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
